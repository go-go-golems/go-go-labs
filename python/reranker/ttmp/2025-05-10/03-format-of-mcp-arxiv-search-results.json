{"results":[{"Title":"MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented Reinforcement in Embodied Systems","Authors":["Anirudh Chari","Suraj Reddy","Aditya Tiwari","Richard Lian","Brian Zhou"],"Abstract":"While large language models (LLMs) have shown promising capabilities as zero-shot planners for embodied agents, their inability to learn from experience and build persistent mental models limits their robustness in complex open-world environments like Minecraft. We introduce MINDSTORES, an experience-augmented planning framework that enables embodied agents to build and leverage mental models through natural interaction with their environment. Drawing inspiration from how humans construct and refine cognitive mental models, our approach extends existing zero-shot LLM planning by maintaining a database of past experiences that informs future planning iterations. The key innovation is representing accumulated experiences as natural language embeddings of (state, task, plan, outcome) tuples, which can then be efficiently retrieved and reasoned over by an LLM planner to generate insights and guide plan refinement for novel states and tasks. Through extensive experiments in the MineDojo environment, a simulation environment for agents in Minecraft that provides low-level controls for Minecraft, we find that MINDSTORES learns and applies its knowledge significantly better than existing memory-based LLM planners while maintaining the flexibility and generalization benefits of zero-shot approaches, representing an important step toward more capable embodied AI systems that can learn continuously through natural experience.","Published":"2025-01-31T17:15:33Z","DOI":"","PDFURL":"http://arxiv.org/pdf/2501.19318v1","SourceURL":"http://arxiv.org/abs/2501.19318v1","SourceName":"arxiv","OAStatus":"green","License":"","FileSize":"","Citations":0,"Type":"","JournalInfo":"","Metadata":{"comment":"","primary_category":"cs.AI","updated":"2025-01-31T17:15:33Z"}},{"Title":"LeanProgress: Guiding Search for Neural Theorem Proving via Proof Progress Prediction","Authors":["Suozhi Huang","Peiyang Song","Robert Joseph George","Anima Anandkumar"],"Abstract":"Mathematical reasoning remains a significant challenge for Large Language Models (LLMs) due to hallucinations. When combined with formal proof assistants like Lean, these hallucinations can be eliminated through rigorous verification, making theorem proving reliable. However, even with formal verification, LLMs still struggle with long proofs and complex mathematical formalizations. While Lean with LLMs offers valuable assistance with retrieving lemmas, generating tactics, or even complete proofs, it lacks a crucial capability: providing a sense of proof progress. This limitation particularly impacts the overall development efficiency in large formalization projects. We introduce LeanProgress, a method that predicts the progress in the proof. Training and evaluating our models made on a large corpus of Lean proofs from Lean Workbook Plus and Mathlib4 and how many steps remain to complete it, we employ data preprocessing and balancing techniques to handle the skewed distribution of proof lengths. Our experiments show that LeanProgress achieves an overall prediction accuracy of 75.1\\% in predicting the amount of progress and, hence, the remaining number of steps. When integrated into a best-first search framework using Reprover, our method shows a 3.8\\% improvement on Mathlib4 compared to baseline performances of 41.2\\%, particularly for longer proofs. These results demonstrate how proof progress prediction can enhance both automated and interactive theorem proving, enabling users to make more informed decisions about proof strategies.","Published":"2025-02-25T07:46:36Z","DOI":"","PDFURL":"http://arxiv.org/pdf/2502.17925v2","SourceURL":"http://arxiv.org/abs/2502.17925v2","SourceName":"arxiv","OAStatus":"green","License":"","FileSize":"","Citations":0,"Type":"","JournalInfo":"","Metadata":{"comment":"","primary_category":"cs.AI","updated":"2025-02-27T17:26:11Z"}},{"Title":"Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models through Axiomatic Causal Interventions","Authors":["Oliver Savolainen","Dur e Najaf Amjad","Roxana Petcu"],"Abstract":"This reproducibility study analyzes and extends the paper \"Axiomatic Causal Interventions for Reverse Engineering Relevance Computation in Neural Retrieval Models,\" which investigates how neural retrieval models encode task-relevant properties such as term frequency. We reproduce key experiments from the original paper, confirming that information on query terms is captured in the model encoding. We extend this work by applying activation patching to Spanish and Chinese datasets and by exploring whether document-length information is encoded in the model as well. Our results confirm that the designed activation patching method can isolate the behavior to specific components and tokens in neural retrieval models. Moreover, our findings indicate that the location of term frequency generalizes across languages and that in later layers, the information for sequence-level tasks is represented in the CLS token. The results highlight the need for further research into interpretability in information retrieval and reproducibility in machine learning research. Our code is available at https://github.com/OliverSavolainen/axiomatic-ir-reproduce.","Published":"2025-05-04T15:30:45Z","DOI":"","PDFURL":"http://arxiv.org/pdf/2505.02154v1","SourceURL":"http://arxiv.org/abs/2505.02154v1","SourceName":"arxiv","OAStatus":"green","License":"","FileSize":"","Citations":0,"Type":"","JournalInfo":"","Metadata":{"comment":"10 pages, SIGIR 2025","primary_category":"cs.IR","updated":"2025-05-04T15:30:45Z"}},{"Title":"Cognitive-Aligned Document Selection for Retrieval-augmented Generation","Authors":["Bingyu Wan","Fuxi Zhang","Zhongpeng Qi","Jiayi Ding","Jijun Li","Baoshi Fan","Yijia Zhang","Jun Zhang"],"Abstract":"Large language models (LLMs) inherently display hallucinations since the precision of generated texts cannot be guaranteed purely by the parametric knowledge they include. Although retrieval-augmented generation (RAG) systems enhance the accuracy and reliability of generative models by incorporating external documents, these retrieved documents often fail to adequately support the model's responses in practical applications. To address this issue, we propose GGatrieval (Fine-\\textbf{G}rained \\textbf{G}rounded \\textbf{A}lignment Re\\textbf{trieval} for verifiable generation), which leverages an LLM to dynamically update queries and filter high-quality, reliable retrieval documents. Specifically, we parse the user query into its syntactic components and perform fine-grained grounded alignment with the retrieved documents. For query components that cannot be individually aligned, we propose a dynamic semantic compensation mechanism that iteratively refines and rewrites the query while continuously updating the retrieval results. This iterative process continues until the retrieved documents sufficiently support the query's response. Our approach introduces a novel criterion for filtering retrieved documents, closely emulating human strategies for acquiring targeted information. This ensures that the retrieved content effectively supports and verifies the generated outputs. On the ALCE benchmark, our method significantly surpasses a wide range of baselines, achieving state-of-the-art performance.","Published":"2025-02-17T13:00:15Z","DOI":"","PDFURL":"http://arxiv.org/pdf/2502.11770v1","SourceURL":"http://arxiv.org/abs/2502.11770v1","SourceName":"arxiv","OAStatus":"green","License":"","FileSize":"","Citations":0,"Type":"","JournalInfo":"","Metadata":{"comment":"","primary_category":"cs.AI","updated":"2025-02-17T13:00:15Z"}},{"Title":"Interpretability Analysis of Domain Adapted Dense Retrievers","Authors":["Goksenin Yuksel","Jaap Kamps"],"Abstract":"Dense retrievers have demonstrated significant potential for neural information retrieval; however, they exhibit a lack of robustness to domain shifts, thereby limiting their efficacy in zero-shot settings across diverse domains. Previous research has investigated unsupervised domain adaptation techniques to adapt dense retrievers to target domains. However, these studies have not focused on explainability analysis to understand how such adaptations alter the model's behavior. In this paper, we propose utilizing the integrated gradients framework to develop an interpretability method that provides both instance-based and ranking-based explanations for dense retrievers. To generate these explanations, we introduce a novel baseline that reveals both query and document attributions. This method is used to analyze the effects of domain adaptation on input attributions for query and document tokens across two datasets: the financial question answering dataset (FIQA) and the biomedical information retrieval dataset (TREC-COVID). Our visualizations reveal that domain-adapted models focus more on in-domain terminology compared to non-adapted models, exemplified by terms such as \"hedge,\" \"gold,\" \"corona,\" and \"disease.\" This research addresses how unsupervised domain adaptation techniques influence the behavior of dense retrievers when adapted to new domains. Additionally, we demonstrate that integrated gradients are a viable choice for explaining and analyzing the internal mechanisms of these opaque neural models.","Published":"2025-01-24T12:42:53Z","DOI":"","PDFURL":"http://arxiv.org/pdf/2501.14459v1","SourceURL":"http://arxiv.org/abs/2501.14459v1","SourceName":"arxiv","OAStatus":"green","License":"","FileSize":"","Citations":0,"Type":"","JournalInfo":"","Metadata":{"comment":"","primary_category":"cs.IR","updated":"2025-01-24T12:42:53Z"}},{"Title":"Graph-Augmented Reasoning: Evolving Step-by-Step Knowledge Graph Retrieval for LLM Reasoning","Authors":["Wenjie Wu","Yongcheng Jing","Yingjie Wang","Wenbin Hu","Dacheng Tao"],"Abstract":"Recent large language model (LLM) reasoning, despite its success, suffers from limited domain knowledge, susceptibility to hallucinations, and constrained reasoning depth, particularly in small-scale models deployed in resource-constrained environments. This paper presents the first investigation into integrating step-wise knowledge graph retrieval with step-wise reasoning to address these challenges, introducing a novel paradigm termed as graph-augmented reasoning. Our goal is to enable frozen, small-scale LLMs to retrieve and process relevant mathematical knowledge in a step-wise manner, enhancing their problem-solving abilities without additional training. To this end, we propose KG-RAR, a framework centered on process-oriented knowledge graph construction, a hierarchical retrieval strategy, and a universal post-retrieval processing and reward model (PRP-RM) that refines retrieved information and evaluates each reasoning step. Experiments on the Math500 and GSM8K benchmarks across six models demonstrate that KG-RAR yields encouraging results, achieving a 20.73\\% relative improvement with Llama-3B on Math500.","Published":"2025-03-03T15:20:41Z","DOI":"","PDFURL":"http://arxiv.org/pdf/2503.01642v1","SourceURL":"http://arxiv.org/abs/2503.01642v1","SourceName":"arxiv","OAStatus":"green","License":"","FileSize":"","Citations":0,"Type":"","JournalInfo":"","Metadata":{"comment":"","primary_category":"cs.AI","updated":"2025-03-03T15:20:41Z"}},{"Title":"Benchmarking Retrieval-Augmented Generation in Multi-Modal Contexts","Authors":["Zhenghao Liu","Xingsheng Zhu","Tianshuo Zhou","Xinyi Zhang","Xiaoyuan Yi","Yukun Yan","Yu Gu","Ge Yu","Maosong Sun"],"Abstract":"This paper introduces Multi-Modal Retrieval-Augmented Generation (M^2RAG), a benchmark designed to evaluate the effectiveness of Multi-modal Large Language Models (MLLMs) in leveraging knowledge from multi-modal retrieval documents. The benchmark comprises four tasks: image captioning, multi-modal question answering, multi-modal fact verification, and image reranking. All tasks are set in an open-domain setting, requiring RAG models to retrieve query-relevant information from a multi-modal document collection and use it as input context for RAG modeling. To enhance the context utilization capabilities of MLLMs, we also introduce Multi-Modal Retrieval-Augmented Instruction Tuning (MM-RAIT), an instruction tuning method that optimizes MLLMs within multi-modal contexts. Our experiments show that MM-RAIT improves the performance of RAG systems by enabling them to effectively learn from multi-modal contexts. All data and code are available at https://github.com/NEUIR/M2RAG.","Published":"2025-02-24T16:25:25Z","DOI":"","PDFURL":"http://arxiv.org/pdf/2502.17297v1","SourceURL":"http://arxiv.org/abs/2502.17297v1","SourceName":"arxiv","OAStatus":"green","License":"","FileSize":"","Citations":0,"Type":"","JournalInfo":"","Metadata":{"comment":"","primary_category":"cs.AI","updated":"2025-02-24T16:25:25Z"}},{"Title":"Retrieval-augmented in-context learning for multimodal large language models in disease classification","Authors":["Zaifu Zhan","Shuang Zhou","Xiaoshan Zhou","Yongkang Xiao","Jun Wang","Jiawen Deng","He Zhu","Yu Hou","Rui Zhang"],"Abstract":"Objectives: We aim to dynamically retrieve informative demonstrations, enhancing in-context learning in multimodal large language models (MLLMs) for disease classification. Methods: We propose a Retrieval-Augmented In-Context Learning (RAICL) framework, which integrates retrieval-augmented generation (RAG) and in-context learning (ICL) to adaptively select demonstrations with similar disease patterns, enabling more effective ICL in MLLMs. Specifically, RAICL examines embeddings from diverse encoders, including ResNet, BERT, BioBERT, and ClinicalBERT, to retrieve appropriate demonstrations, and constructs conversational prompts optimized for ICL. We evaluated the framework on two real-world multi-modal datasets (TCGA and IU Chest X-ray), assessing its performance across multiple MLLMs (Qwen, Llava, Gemma), embedding strategies, similarity metrics, and varying numbers of demonstrations. Results: RAICL consistently improved classification performance. Accuracy increased from 0.7854 to 0.8368 on TCGA and from 0.7924 to 0.8658 on IU Chest X-ray. Multi-modal inputs outperformed single-modal ones, with text-only inputs being stronger than images alone. The richness of information embedded in each modality will determine which embedding model can be used to get better results. Few-shot experiments showed that increasing the number of retrieved examples further enhanced performance. Across different similarity metrics, Euclidean distance achieved the highest accuracy while cosine similarity yielded better macro-F1 scores. RAICL demonstrated consistent improvements across various MLLMs, confirming its robustness and versatility. Conclusions: RAICL provides an efficient and scalable approach to enhance in-context learning in MLLMs for multimodal disease classification.","Published":"2025-05-04T12:43:56Z","DOI":"","PDFURL":"http://arxiv.org/pdf/2505.02087v1","SourceURL":"http://arxiv.org/abs/2505.02087v1","SourceName":"arxiv","OAStatus":"green","License":"","FileSize":"","Citations":0,"Type":"","JournalInfo":"","Metadata":{"comment":"17 Pages, 1 figure, 7 tables","primary_category":"cs.AI","updated":"2025-05-04T12:43:56Z"}},{"Title":"Empowering GraphRAG with Knowledge Filtering and Integration","Authors":["Kai Guo","Harry Shomer","Shenglai Zeng","Haoyu Han","Yu Wang","Jiliang Tang"],"Abstract":"In recent years, large language models (LLMs) have revolutionized the field of natural language processing. However, they often suffer from knowledge gaps and hallucinations. Graph retrieval-augmented generation (GraphRAG) enhances LLM reasoning by integrating structured knowledge from external graphs. However, we identify two key challenges that plague GraphRAG:(1) Retrieving noisy and irrelevant information can degrade performance and (2)Excessive reliance on external knowledge suppresses the model's intrinsic reasoning. To address these issues, we propose GraphRAG-FI (Filtering and Integration), consisting of GraphRAG-Filtering and GraphRAG-Integration. GraphRAG-Filtering employs a two-stage filtering mechanism to refine retrieved information. GraphRAG-Integration employs a logits-based selection strategy to balance external knowledge from GraphRAG with the LLM's intrinsic reasoning,reducing over-reliance on retrievals. Experiments on knowledge graph QA tasks demonstrate that GraphRAG-FI significantly improves reasoning performance across multiple backbone models, establishing a more reliable and effective GraphRAG framework.","Published":"2025-03-18T01:29:55Z","DOI":"","PDFURL":"http://arxiv.org/pdf/2503.13804v1","SourceURL":"http://arxiv.org/abs/2503.13804v1","SourceName":"arxiv","OAStatus":"green","License":"","FileSize":"","Citations":0,"Type":"","JournalInfo":"","Metadata":{"comment":"","primary_category":"cs.AI","updated":"2025-03-18T01:29:55Z"}},{"Title":"MMKB-RAG: A Multi-Modal Knowledge-Based Retrieval-Augmented Generation Framework","Authors":["Zihan Ling","Zhiyao Guo","Yixuan Huang","Yi An","Shuai Xiao","Jinsong Lan","Xiaoyong Zhu","Bo Zheng"],"Abstract":"Recent advancements in large language models (LLMs) and multi-modal LLMs have been remarkable. However, these models still rely solely on their parametric knowledge, which limits their ability to generate up-to-date information and increases the risk of producing erroneous content. Retrieval-Augmented Generation (RAG) partially mitigates these challenges by incorporating external data sources, yet the reliance on databases and retrieval systems can introduce irrelevant or inaccurate documents, ultimately undermining both performance and reasoning quality. In this paper, we propose Multi-Modal Knowledge-Based Retrieval-Augmented Generation (MMKB-RAG), a novel multi-modal RAG framework that leverages the inherent knowledge boundaries of models to dynamically generate semantic tags for the retrieval process. This strategy enables the joint filtering of retrieved documents, retaining only the most relevant and accurate references. Extensive experiments on knowledge-based visual question-answering tasks demonstrate the efficacy of our approach: on the E-VQA dataset, our method improves performance by +4.2% on the Single-Hop subset and +0.4% on the full dataset, while on the InfoSeek dataset, it achieves gains of +7.8% on the Unseen-Q subset, +8.2% on the Unseen-E subset, and +8.1% on the full dataset. These results highlight significant enhancements in both accuracy and robustness over the current state-of-the-art MLLM and RAG frameworks.","Published":"2025-04-14T10:19:47Z","DOI":"","PDFURL":"http://arxiv.org/pdf/2504.10074v3","SourceURL":"http://arxiv.org/abs/2504.10074v3","SourceName":"arxiv","OAStatus":"green","License":"","FileSize":"","Citations":0,"Type":"","JournalInfo":"","Metadata":{"comment":"","primary_category":"cs.AI","updated":"2025-04-20T17:16:02Z"}}]}